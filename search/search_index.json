{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"skaff-rag-accelerator","text":"<p>This is a starter kit to deploy a modularizable RAG locally or on the cloud (or across multiple clouds)</p>"},{"location":"#features","title":"Features","text":"<ul> <li>A configurable RAG setup based around Langchain (Check out the configuration cookbook here)</li> <li><code>RAG</code> and <code>RagConfig</code> python classes that manage components (vector store, llm, retreiver, ...)</li> <li>A REST API based on Langserve + FastAPI to provide easy access to the RAG as a web backend</li> <li>Optional API plugins for secure user authentication, session management, ...</li> <li><code>Chain links</code> primitive that facilitates chain building and allows documentation generation</li> <li>A demo Streamlit to serve as a basic working frontend</li> <li><code>Dockerfiles</code> and <code>docker-compose</code> to make deployments easier and more flexible</li> <li>A document loader for the RAG</li> </ul>"},{"location":"#quickstart","title":"Quickstart","text":"<p>This quickstart will guide you through the steps to serve the RAG and load a few documents.</p> <p>You will run both the back and front on your machine.</p> <p>For this exemple, we will be using GPT4, the <code>BAAI/bge-base-en-v1.5</code> embedding model, and Chroma for the vector store.</p> <p>Duration: ~15 minutes.</p>"},{"location":"#pre-requisites","title":"Pre-requisites","text":"<ul> <li>An <code>OPENAI_API_KEY</code> for the Artefact GPT-4 deployment on Azure. Contact alexis.vialaret@artefact.com if you do not have one.</li> <li>A few GB of disk space</li> <li>Tested with python 3.11 (may work with other versions)</li> </ul>"},{"location":"#run-using-docker-compose","title":"Run using docker compose","text":"<p>If you have docker installed and running you can run the whole RAG app using it. Otherwise, skip to the \"Run directly\" section</p> <p>Start the service: <pre><code>docker compose up -d\n</code></pre></p> <p>Make sure both the front and back are alive: <pre><code>docker ps\n</code></pre> You should see two containers with status <code>Up X minutes</code>.</p> <p>Go to http://localhost:9000/ to query your RAG.</p>"},{"location":"#run-directly","title":"Run directly","text":"<p>In a fresh env: <pre><code>pip install -r requirements-dev.txt\n</code></pre></p> <p>You will need to set some env vars, either in a .env file at the project root, or just by exporting them like so: <pre><code>export PYTHONPATH=.\nexport ADMIN_MODE=1\n</code></pre></p> <p>Start the backend server locally: <pre><code>python -m uvicorn backend.main:app\n</code></pre></p> <p>Start the frontend demo <pre><code>python -m streamlit run frontend/front.py\n</code></pre></p>"},{"location":"#loading-documents-in-the-rag","title":"Loading documents in the RAG","text":"<p>Right now the RAG does not have any documents loaded, you can use the notebook in the <code>examples</code> folder to transform a file into documents and load them in the vector store.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>To deep dive into under the hood, take a look at the documentation</p> <p>On github pages</p> <p>Or serve them locally: <pre><code>mkdocs serve\n</code></pre> Then go to http://localhost:8000/</p>"},{"location":"#architecture","title":"Architecture","text":"<p>The whole goal of this repo is to decouple the \"computing and LLM querying\" part from the \"rendering a user interface\" part. We do this with a typical 3-tier architecture.</p> <p></p> <ul> <li>The frontend is the end user facing part. It reches out to the backend ONLY through the REST API. We provide a frontend demo here for convenience, but ultimately it could live in a completely different repo, and be written in a completely different language.</li> <li>The backend provides a REST API to abstract RAG functionalities. It handles calls to LLMs, tracks conversations and users, handles the state management using a db, and much more. To get the gist of the backend, look at the of the API: http://0.0.0.0:8000/docs. It can be extended by plugins.</li> <li>The database is only accessed by the backend and persists the state of the RAG application. The same plugins that extend the functionalities of the backed, can extend the data model of the DB.</li> </ul> <p>The structure of the repo mirrors this architecture.</p> <p></p> <p>The RAG itself does the following: </p>"},{"location":"database/","title":"The database","text":"<p>The backend is stateless, it does not retain information about previous requests. There is no stored knowledge of, or reference to past transactions in the backend itself. Instead, all this state knowledge is handled by a database.</p> <p>In the medium/long term, stateless backends are a much simpler paradigm. They are much easier to develop, test, and debug since all requests are self-contained, with all the side effects are stored on the DB. They also scale to multiple instances much better since any query can be responded to by any instance of the backend.</p>"},{"location":"database/#choosing-the-right-database-backend","title":"Choosing the right Database backend.","text":"<p>This accelerator supports <code>SQLite</code>, <code>Postgres</code>, and <code>MySQL</code></p> <p>This section focuses on architecture descision making. you will find implementation details and recipes here in the cookbook.</p>"},{"location":"database/#sqlite","title":"SQLite","text":"<p>Pick this if</p> <ul> <li>You only want to do small scale prototyping</li> <li>You are working alone</li> <li>This is the very beginning of the project</li> </ul> <p>SQLite is the default option. It is a minimalist SQL database stored as a single <code>.sqlite</code> file. This is suitable for local development and prototyping, but not for industrialization.</p> <p>As the data is only persisted locally, this also means you can not easily share it with the rest of the dev team in your project. If that is something you need, consider using cloud-based Postgres or MySQL backends.</p> <p>Visualizing the DB in your IDE</p> <p>In VSCode/cursor, the <code>SQLite Viewer</code> extension is very useful for dev and debug as it allows you to open and explore the <code>.sqlite</code> database file.</p>"},{"location":"database/#postgres","title":"Postgres","text":"<p>Pick this if</p> <ul> <li>You have multiple people working with you</li> <li>Your project will require some form of industrialization</li> <li>You also want to use Postgres as your vector store (using <code>pgvector</code>)</li> </ul> <p>Postgres is the preferred choice for any project that has the ambition to go beyond a simple POC. All clouds offer a managed Postgres so it is usually pretty easy and cheap to deploy and use.</p> <p>This centralized database can be queried from anywhere by any backend instance provided they have access. Very useful for working as a team and sharing data such as chat sessions, feedbacks, or other. Mandatory to serve a service that will scale beyond 10 users.</p> <p>By using Postgres you can kill two birds with one stone as it can also be used as your vector store. That way, you essentially get a production-grade vector store for free as you need a SQL backend anyways.</p> <p>Visualizing the DB in your IDE</p> <p>In VSCode/cursor, the <code>PostgresSQL</code> extension is very useful for dev and debug as it allows you to open and explore the remote database.</p>"},{"location":"database/#mysql","title":"MySQL","text":"<p>Pick this if</p> <ul> <li>You can not use Postgres or if you already have a MySQL database</li> <li>You have multiple people working with you</li> <li>Your project will require some form of industrialization</li> </ul> <p>This is essentially the same as postgres, but without the possibility of doubling as a vector store. Unless there is a very project-specific reason to use it, just prefer other options.</p>"},{"location":"database/#interacting-with-the-database","title":"Interacting with the database","text":"<p>Only the backend interacts with the database, not the frontend.</p> <p></p> <p>All the databases interactions should go through the <code>Database</code> helper class at backend/database.py</p> <p>This class abstracts away the underlying database and allows you to run SQL queries while handling connections securely and efficiently in the background.</p> <p>For example: fetching a user by email can be done like this: <pre><code>from backend.database import Database\n\nwith Database() as connection:\n    user_row = connection.fetchone(\"SELECT * FROM users WHERE email = ?\", (email,))\n</code></pre></p>"},{"location":"database/#database-data-model","title":"Database data model","text":"<p>The minimal database for the RAG only has one table, <code>message_history</code>. It is meant to be extended by plugins to add functionalities as they are needed. See the the plugins documentation for more info.</p>"},{"location":"frontend/","title":"The frontend","text":"<p>The frontend is the end user facing part. It reaches out to the backend ONLY through the REST API. We provide a Streamlit frontend demo here for convenience, but ultimately it could live in a completely different repo, and be written in a completely different language.</p> <p>As you work on this repo, it is advisable to keep the front and back decoupled. You can consider the <code>backend</code> and <code>frontend</code> folders to be two different, standalone repos.</p> <p>You may have code that looks like it would fit well in a <code>commons</code> directory and be used by both. In that case, prefer integrating it in the backend and making it available to the frontend via the API. You can also just duplicate the code if it's small enough.</p>"},{"location":"backend/backend/","title":"The Backend","text":"<p>The backend provides a REST API to abstract RAG functionalities. The core embarks just enough to query your indexed documents.</p> <p>More advanced features (authentication, user sessions, ...) can be enabled through plugins.</p>"},{"location":"backend/backend/#architecture","title":"Architecture","text":"<p>Start the backend server locally: <pre><code>python -m uvicorn backend.main:app\n</code></pre></p> <p>INFO:     Application startup complete.</p> <p>INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)</p>"},{"location":"backend/backend/#base-rag","title":"Base RAG","text":"<p>The base RAG-as-a-service API is defined at <code>backend/main.py</code>: <pre><code>rag = RAG(config=Path(__file__).parent / \"config.yaml\")\nchain = rag.get_chain()\n\napp = FastAPI(\n    title=\"RAG Accelerator\",\n    description=\"A RAG-based question answering API\",\n)\n\nadd_routes(app, chain)\n</code></pre> The basic core RAG allows you to load and ask questions about documents. <code>add_routes</code> comes straight from Langserve and sets up the basing API routes for chain serving. Our plugins will be added similarly.</p> <p>By going to the API documentation (http://0.0.0.0:8000/docs if serving locally) you will have these routes. You can query your RAG directly from here using the <code>/invoke</code> endpoint if you want to.</p> <p></p> <p></p> <p>You can also query your RAG using the Langserve playground at http://0.0.0.0:8000/playground. It should look like this:</p> <p></p>"},{"location":"backend/rag_ragconfig/","title":"RAG and RAGConfig classes","text":""},{"location":"backend/rag_ragconfig/#the-rag-class","title":"The <code>RAG</code> class","text":"<p>The RAG class orchestrates the components necessary for a retrieval-augmented generation pipeline. It initializes with a configuration, either directly or from a file.</p> <p></p> <p>The RAG object has two main purposes:</p> <ul> <li>loading the RAG with documents, which involves ingesting and processing documents to be retrievable by the system</li> <li>generating the chain from the components as specified in the configuration, which entails assembling the various components (language model, embeddings, vector store) into a coherent pipeline for generating responses based on retrieved information.</li> </ul> <p>Loading and querying documents</p> <pre><code>from pathlib import Path\nfrom backend.rag_components.rag import RAG\n\nrag = RAG(config=Path(__file__).parent / \"backend\" / \"config.yaml\")\nchain = rag.get_chain()\n\nprint(chain.invoke(\"Who is bill Gates?\"))\n# &gt; content='Documents have not been provided, and thus I am unable to give a response based on them. Would you like me to answer based on general knowledge instead?'\n\nrag.load_file(Path(__file__).parent / \"data_sample\" / \"billionaires.csv\")\n# &gt; loader selected CSVLoader for /.../data_sample/billionaires.csv\n# &gt; {'event': 'load_documents', 'num_added': 2640, 'num_updated': 0, 'num_skipped': 0, 'num_deleted': 0}\n\nprint(chain.invoke(\"Who is bill Gates?\"))\n# &gt; content='Bill Gates is a 67-year-old businessman from the United States, residing in Medina, Washington. He is the co-chair of the Bill &amp; Melinda Gates Foundation and is recognized for his self-made success, primarily through Microsoft in the technology industry. As of the provided document dated April 4, 2023, Bill Gates has a final worth of $104 billion, ranking him 6th in the category of Technology. His full name is William Gates, and he was born on October 28, 1955.'\n</code></pre>"},{"location":"backend/rag_ragconfig/#ragconfig","title":"<code>RAGConfig</code>","text":"<p>Configuration of the RAG is done using the <code>RAGConfig</code> dataclass. You can instanciate one directly in python, but the preferred way is to use the <code>backend/config.yaml</code> file. This YAML is then automatically parsed into a <code>RAGConfig</code> that can be fed to the <code>RAG</code> class.</p> <p>The configuration provides you with a way to input which implementation you want to use for each RAG components:</p> <ul> <li>The LLM</li> <li>The embedding model</li> <li>The vector store / retreiver</li> <li>The memory / database</li> </ul> <p>Zooming in on the <code>LLMConfig</code> as an example: <pre><code>@dataclass\nclass LLMConfig:\n    source: BaseChatModel | LLM | str\n    source_config: dict\n    temperature: float\n</code></pre></p> <ul> <li><code>source</code> is the name of name of the langchain class name of your model, either a <code>BaseChatModel</code> or <code>LLM</code>.</li> <li><code>source_config</code> is are the parameters used to instanciate the <code>source</code>.</li> <li><code>temperature</code> regulates the unpredictability of a language model's output.</li> </ul> <p>Example of a configuration that uses a local model served with Ollama. In <code>backend/config.yaml</code>: <pre><code>LLMConfig: &amp;LLMConfig\n  source: ChatOllama\n  source_config:\n    model: tinyllama\n    temperature: 0\n</code></pre></p> <p>Configuration recipes</p> <p>You can find fully tested recipes for LLMConfig, VectorStoreConfig, EmbeddingModelConfig, and DatabaseConfig in the Cookbook.</p> <p>This is the python equivalent that is generated and executed under the hood when a <code>RAG</code> object is created. <pre><code>llm = ChatOllama(model=\"tinyllama\", temperature=0)\n</code></pre></p> <p>You can also write the configurations directly in python, although that's not the recommended approach here. <pre><code>from langchain_community.chat_models import ChatOllama\n\nfrom backend.config import LLMConfig\n\nllm_config = LLMConfig(\n    source=ChatOllama,\n    source_config={\"model\": \"llama2\", \"temperature\": 0},\n)\n</code></pre></p>"},{"location":"backend/rag_ragconfig/#extending-the-ragconfig","title":"Extending the <code>RAGConfig</code>","text":"<p>See: How to extend the RAGConfig</p>"},{"location":"backend/chains/basic_chain/","title":"Basic chain","text":""},{"location":"backend/chains/basic_chain/#answer-questions-from-documents-stored-in-a-vector-store","title":"Answer questions from documents stored in a vector store","text":"<p>This chain answers the provided question based on documents it retreives.</p>"},{"location":"backend/chains/basic_chain/#prompt","title":"Prompt","text":"<pre><code>As a chatbot assistant, your mission is to respond to user inquiries in a precise and concise manner based on the documents provided as input. It is essential to respond in the same language in which the question was asked. Responses must be written in a professional style and must demonstrate great attention to detail. Do not invent information. You must sift through various sources of information, disregarding any data that is not relevant to the query's context. Your response should integrate knowledge from the valid sources you have identified. Additionally, the question might include hypothetical or counterfactual statements. You need to recognize these and adjust your response to provide accurate, relevant information without being misled by the counterfactuals. Respond to the question only taking into account the following context. If no context is provided, do not answer. You may provide an answer if the user explicitely asked for a general answer. You may ask the user to rephrase their question, or their permission to answer without specific context from your own knowledge.\nContext: {relevant_documents}\n\nQuestion: {question}\n</code></pre>"},{"location":"backend/chains/basic_chain/#input-str","title":"Input: str","text":""},{"location":"backend/chains/basic_chain/#output-response","title":"Output: Response","text":"Name Type Required Default response str True"},{"location":"backend/chains/basic_chain/#sub-chain","title":"Sub-chain","text":"Fetch documents"},{"location":"backend/chains/basic_chain/#fetch-documents","title":"Fetch documents","text":"<p>This chain fetches the relevant documents and combines them into a single string.</p>"},{"location":"backend/chains/basic_chain/#prompt_1","title":"Prompt","text":"<pre><code>{page_content}\n</code></pre>"},{"location":"backend/chains/basic_chain/#input-question","title":"Input: Question","text":"Name Type Required Default question str True"},{"location":"backend/chains/basic_chain/#output-documents","title":"Output: Documents","text":"Name Type Required Default documents str True"},{"location":"backend/chains/chain_with_memory/","title":"Chain with memory","text":""},{"location":"backend/chains/chain_with_memory/#rag-with-persistant-memory","title":"RAG with persistant memory","text":"<p>This chain answers the provided question based on documents it retreives and the conversation history. It uses a persistant memory to store the conversation history.</p>"},{"location":"backend/chains/chain_with_memory/#input-dict","title":"Input: Dict","text":""},{"location":"backend/chains/chain_with_memory/#output-response","title":"Output: Response","text":"Name Type Required Default response str True"},{"location":"backend/chains/chain_with_memory/#sub-chain","title":"Sub-chain","text":"Answer question from docs and history"},{"location":"backend/chains/chain_with_memory/#answer-question-from-docs-and-history","title":"Answer question from docs and history","text":"<p>This chain answers the provided question based on documents it retreives and the conversation history</p>"},{"location":"backend/chains/chain_with_memory/#input-questionwithhistory","title":"Input: QuestionWithHistory","text":"Name Type Required Default question str True chat_history str True"},{"location":"backend/chains/chain_with_memory/#output-response_1","title":"Output: Response","text":"Name Type Required Default response str True"},{"location":"backend/chains/chain_with_memory/#sub-chain_1","title":"Sub-chain","text":"RunnableSequence"},{"location":"backend/chains/chain_with_memory/#runnablesequence","title":"RunnableSequence","text":""},{"location":"backend/chains/chain_with_memory/#input-questionwithchathistory","title":"Input: QuestionWithChatHistory","text":"Name Type Required Default question str True chat_history str True"},{"location":"backend/chains/chain_with_memory/#output-response_2","title":"Output: Response","text":"Name Type Required Default response str True"},{"location":"backend/chains/chain_with_memory/#these-chains-run-in-sequence","title":"These chains run in sequence","text":"Condense question and history Answer questions from documents stored in a vector store"},{"location":"backend/chains/chain_with_memory/#condense-question-and-history","title":"Condense question and history","text":"<p>This chain condenses the chat history and the question into one standalone question.</p>"},{"location":"backend/chains/chain_with_memory/#prompt","title":"Prompt","text":"<pre><code>&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\nGiven the conversation history and the following question, can you rephrase the user's question in its original language so that it is self-sufficient. You are presented with a conversation that may contain some spelling mistakes and grammatical errors, but your goal is to understand the underlying question. Make sure to avoid the use of unclear pronouns.\n\nIf the question is already self-sufficient, return the original question. If it seem the user is authorizing the chatbot to answer without specific context, make sure to reflect that in the rephrased question.\n&lt;&lt;/SYS&gt;&gt;\n\nChat history: {chat_history}\n\nQuestion: {question}\n[/INST]\n</code></pre>"},{"location":"backend/chains/chain_with_memory/#input-questionwithchathistory_1","title":"Input: QuestionWithChatHistory","text":"Name Type Required Default question str True chat_history str True"},{"location":"backend/chains/chain_with_memory/#output-standalonequestion","title":"Output: StandaloneQuestion","text":"Name Type Required Default standalone_question str True"},{"location":"backend/chains/chain_with_memory/#answer-questions-from-documents-stored-in-a-vector-store","title":"Answer questions from documents stored in a vector store","text":"<p>This chain answers the provided question based on documents it retreives.</p>"},{"location":"backend/chains/chain_with_memory/#prompt_1","title":"Prompt","text":"<pre><code>As a chatbot assistant, your mission is to respond to user inquiries in a precise and concise manner based on the documents provided as input. It is essential to respond in the same language in which the question was asked. Responses must be written in a professional style and must demonstrate great attention to detail. Do not invent information. You must sift through various sources of information, disregarding any data that is not relevant to the query's context. Your response should integrate knowledge from the valid sources you have identified. Additionally, the question might include hypothetical or counterfactual statements. You need to recognize these and adjust your response to provide accurate, relevant information without being misled by the counterfactuals. Respond to the question only taking into account the following context. If no context is provided, do not answer. You may provide an answer if the user explicitely asked for a general answer. You may ask the user to rephrase their question, or their permission to answer without specific context from your own knowledge.\nContext: {relevant_documents}\n\nQuestion: {question}\n</code></pre>"},{"location":"backend/chains/chain_with_memory/#input-str","title":"Input: str","text":""},{"location":"backend/chains/chain_with_memory/#output-response_3","title":"Output: Response","text":"Name Type Required Default response str True"},{"location":"backend/chains/chain_with_memory/#sub-chain_2","title":"Sub-chain","text":"Fetch documents"},{"location":"backend/chains/chain_with_memory/#fetch-documents","title":"Fetch documents","text":"<p>This chain fetches the relevant documents and combines them into a single string.</p>"},{"location":"backend/chains/chain_with_memory/#prompt_2","title":"Prompt","text":"<pre><code>{page_content}\n</code></pre>"},{"location":"backend/chains/chain_with_memory/#input-question","title":"Input: Question","text":"Name Type Required Default question str True"},{"location":"backend/chains/chain_with_memory/#output-documents","title":"Output: Documents","text":"Name Type Required Default documents str True"},{"location":"backend/chains/chains/","title":"Chains and chain links","text":"<p>We provide two basic RAG chains to get you started, one does simple one-shot Q&amp;A on your RAG, and the other is able handle conversation history.</p> <p>Basic chain documentation</p> <p>Chain with memory documentation</p>"},{"location":"backend/chains/chains/#chains-and-chain-links","title":"Chains and chain links","text":"<p>This repo does not define large monolithic chains. To make it easier to pick and chose the required functionalities, we provide \"chain links\" at <code>backend/rag_components/chain_links</code>. All links are valid, self-sufficient chains. You can think of it as a toolbox of langchain components meant to be composed and stacked together to build actually useful chains.</p> <p>As all links are <code>Runnable</code> objects, they can be built from other chain links which are themselves made of chain links, etc...</p> <p>This abstraction also makes it very easy to contribute now functionalities, as you can just develop a new chain link and add it here as a file.</p> <p>Typically, a link has:</p> <ul> <li>Input and output pydantic models</li> <li>A prompt</li> <li>A chain definition</li> </ul> <p>For example the <code>condense_question</code> chain link has <code>QuestionWithChatHistory</code> and <code>StandaloneQuestion</code> as input an output models. <pre><code>class QuestionWithChatHistory(BaseModel):\n    question: str\n    chat_history: str\n\n\nclass StandaloneQuestion(BaseModel):\n    standalone_question: str\n</code></pre></p> <p>It also has a prompt that uses the contents of <code>QuestionWithChatHistory</code> to formulate a question based on the chat history and the latest question. <pre><code>prompt = \"\"\"\\\nGiven the conversation history and the following question, can you rephrase the user's question in its original language so that it is self-sufficient. You are presented with a conversation that may contain some spelling mistakes and grammatical errors, but your goal is to understand the underlying question. Make sure to avoid the use of unclear pronouns.\n\nIf the question is already self-sufficient, return the original question. If it seem the user is authorizing the chatbot to answer without specific context, make sure to reflect that in the rephrased question.\n\nChat history: {chat_history}\n\nQuestion: {question}\n\"\"\" # noqa: E501\n</code></pre></p> <p>And finally a function that defines and returns the actual chain. Notice the <code>with_types</code> method that binds the pydantic models we defined ealier to the chains inputs and outputs. This is very useful to track your chain's execution and debug. The <code>DocumentedRunnable</code> object will be introduced and explained in the next section. <pre><code>def condense_history_chain(llm) -&gt; DocumentedRunnable:\n    condense_question_prompt = PromptTemplate.from_template(prompt)  # chat_history, question\n\n    standalone_question = condense_question_prompt | llm | StrOutputParser()\n    typed_chain = standalone_question.with_types(input_type=QuestionWithChatHistory, output_type=StandaloneQuestion)\n\n    return DocumentedRunnable(typed_chain, chain_name=\"Condense question and history\", prompt=prompt, user_doc=__doc__)\n</code></pre></p>"},{"location":"backend/chains/chains/#automated-chain-documentation","title":"Automated chain documentation","text":"<p>As the chains stack can get quite high and complex, it is useful to be able to explain and explore it. This is the goal of the <code>DocumentedRunnable</code>. This binds to a chain and generates markdown documentation from the input/outputs models, and other info you provide it: a prompt, or other documentation.</p> <p>Documentation is recursively generated from all the <code>DocumentedRunnable</code> chains and sub-chains. Sub-chains documentation is nested in the top-level documentation.</p> <p>Experimental</p> <p>This feature is experimental and may have small quirks and bugs. Please do report them to alexis.vialaret@artefact.com so I can fix them. Contributions and feedback are also welcome.</p> <p>In order to document your chain, just wrap it in a <code>DocumentedRunnable</code>: <pre><code>documented_chain = DocumentedRunnable(\n    runnable=my_chain,\n    chain_name=\"My documented Chain\",\n    user_doc=\"Additional chain explainations that will be displayed in the markdown\",\n    prompt=prompt,\n)\n</code></pre></p> <p>For example, let's generate documentation for the <code>condense_question</code> chain link:</p> <pre><code>from backend.rag_components.chain_links.condense_question import condense_question\nfrom langchain.llms.openai import OpenAIChat\n\nllm = OpenAIChat()\nchain = condense_question(llm)\n\nprint(chain.documentation)\n</code></pre> Generated documentation"},{"location":"backend/chains/chains/#condense-question-and-history","title":"Condense question and history","text":"<p>This chain condenses the chat history and the question into one standalone question.</p>"},{"location":"backend/chains/chains/#prompt","title":"Prompt","text":"<pre><code>Given the conversation history and the following question, can you rephrase the user's question in its original language so that it is self-sufficient. You are presented with a conversation that may contain some spelling mistakes and grammatical errors, but your goal is to understand the underlying question. Make sure to avoid the use of unclear pronouns.\n\nIf the question is already self-sufficient, return the original question. If it seem the user is authorizing the chatbot to answer without specific context, make sure to reflect that in the rephrased question.\n\nChat history: {chat_history}\n\nQuestion: {question}\n</code></pre>"},{"location":"backend/chains/chains/#input-questionwithchathistory","title":"Input: QuestionWithChatHistory","text":"Name Type Required Default question str True chat_history str True"},{"location":"backend/chains/chains/#output-standalonequestion","title":"Output: StandaloneQuestion","text":"Name Type Required Default standalone_question str True"},{"location":"backend/plugins/authentication/","title":"Authentication","text":"<p>We provide two plugins for user management: secure, and insecure authentication.</p> <ul> <li>The secure authentication is the recommended approach when the API is intended to be deployed on a public endpoint.</li> <li>The insecure plugin is a little simpler and can be used in testing or when users having access to other people's sessions is not an issue.</li> </ul> <p>Do not use the insecure auth plugin for deployments exposed to the internet</p> <p>This would allow anyone to query your LLM and spend your tokens.</p> <p><pre><code>from backend.api_plugins import insecure_authentication_routes\n</code></pre> <pre><code>rag = RAG(config=Path(__file__).parent / \"config.yaml\")\nchain = rag.get_chain()\n\napp = FastAPI(\n    title=\"RAG Accelerator\",\n    description=\"A RAG-based question answering API\",\n)\n\nauth = insecure_authentication_routes(app)\nadd_routes(app, chain, dependencies=[auth])\n</code></pre></p> <p>Similarly than for the sesions before, we add the routes that will allow the users to sign up and login using the <code>insecure_authentication_routes</code> plugin.</p> <p>The tricky part is that we need all the existing endpoints to covered by the authentication. To do this we inject <code>auth</code> as a dependency of Langchain's <code>add_routes</code>.</p> <p>We have new user management routes: </p> <p>And now every other route expects an email as a parameter which can be used to retrieve previous chats for examples. </p>"},{"location":"backend/plugins/conversational_rag_plugin/","title":"Conversational rag plugin","text":""},{"location":"backend/plugins/conversational_rag_plugin/#conversational-rag","title":"Conversational RAG","text":"<p>Let's say you want to have sessions support for the RAG to be able hold a conversation rather than just answer standalone questions: <pre><code>from backend.api_plugins import session_routes\n</code></pre> <pre><code>rag = RAG(config=Path(__file__).parent / \"config.yaml\")\nchain = rag.get_chain(memory=True)\n\napp = FastAPI(\n    title=\"RAG Accelerator\",\n    description=\"A RAG-based question answering API\",\n)\n\nadd_routes(app, chain)\nsession_routes(app)\n</code></pre></p> <p>We have added two things here:</p> <ul> <li>We set <code>memory=True</code> in <code>RAG.get_chain</code>. That will create a slightly different chain than before. This new chain adds memory handling capabilities to our RAG.</li> <li>We imported and called the <code>session_routes</code> plugin.</li> </ul> <p>We will now have new session management routes available in the API: </p> <p>And also, the playground now takes a <code>SESSION ID</code> configuration: </p>"},{"location":"backend/plugins/plugins/","title":"Plugins","text":"<p>Plugins are used to add functionalities to the API as you need them.</p> <p>We provide a few plugins out of the box in <code>backend/api_plugins</code>, and you will also be able to create your own. If you write a useful plugin, don't hesitate to open a PR!</p> <p>A plugin takes the form of a function that wraps all the FastAPI routes it introduces.</p>"},{"location":"backend/plugins/plugins/#data-model","title":"Data model","text":"<p>Plugins may need special database tables to properly function. You can bundle a SQL script that will add this table if it dosen't exist when the plugin is instantiated. For example, the authentication plugins adds a table that stores users.</p> <p><code>users_tables.sql</code>: <pre><code>CREATE TABLE IF NOT EXISTS \"users\" (\n    \"email\" VARCHAR(255) PRIMARY KEY,\n    \"password\" TEXT\n);\n</code></pre> <pre><code>def authentication_routes(app, dependencies=List[Depends]):\n    from backend.database import Database\n    with Database() as connection:\n        connection.run_script(Path(__file__).parent / \"users_tables.sql\")\n\n    # rest of the plugin\n</code></pre></p>"},{"location":"backend/plugins/plugins/#dependencies","title":"Dependencies","text":"<p>Plugins should allow for dependency injection. In practice that means the wrapper function should accept a list of FastAPI <code>Depends</code> object and pass it to all the wrapped routes. For example, the sessions plugin takes an unspecified list of dependencies that may be needed in the future, and an explicit auth dependency to link sessions to users. Learn more about FastAPI dependencies here.</p>"},{"location":"backend/plugins/user_based_sessions/","title":"Secure user-based sessions","text":"<p>Now we bring it all together: sessions, and secure authentication. By combining the sessions plugin and the secure authentication plugin, we can have user-specific profiles which are completely distinct from one another.</p> <p><pre><code>from backend.api_plugins import session_routes, authentication_routes\n</code></pre> <pre><code>rag = RAG(config=Path(__file__).parent / \"config.yaml\")\nchain = rag.get_chain(memory=True)\n\napp = FastAPI(\n    title=\"RAG Accelerator\",\n    description=\"A RAG-based question answering API\",\n)\n\nauth = authentication_routes(app)\nsession_routes(app, authentication=auth)\nadd_routes(app, chain, dependencies=[auth])\n</code></pre></p> <p>Here our authentication plugin is injected in both the sessions and core routes. With this setup, all calls will need to be authenticated with a bearer token that the API provides after a sucessful login.</p> <p>Notice the locks pictograms on every route. These indicate the routes are protected by our authentication scheme. You can still query your RAG using this interface by first login through the <code>Authorize</code> button. The Langserve playground does not support this however, and is not usable anymore. </p>"},{"location":"cookbook/cookbook/","title":"Cookbook","text":"<p>Here you will find a repository of configurations that have proven to work.</p> <ul> <li>LLM Configuration samples</li> <li>Embedding model Configuration samples</li> <li>Vector Store Configuration samples</li> <li>Database Configuration samples</li> </ul>"},{"location":"cookbook/extend_ragconfig/","title":"How to extend the RAGConfig","text":"<p>As you tune this starter kit to your needs, you may need to add specific configuration that your RAG will use.</p> <p>For example, let's say you want to add the <code>foo</code> configuration parameter to your vector store configuration.</p> <p>First, add it to <code>config.py</code> in the part relavant to the vector store:</p> <pre><code># ...\n\n@dataclass\nclass VectorStoreConfig:\n    # ... rest of the VectorStoreConfig ...\n\n    foo: str = \"bar\"  # We add foo param, of type str, with the default value \"bar\"\n\n# ...\n</code></pre> <p>This parameter will now be available in your <code>RAG</code> object configuration.</p> <pre><code>from pathlib import Path\nfrom backend.rag_components.rag import RAG\n\nconfig_directory = Path(\"backend/config.yaml\")\nrag = RAG(config_directory)\n\nprint(rag.config.vector_store.foo)\n# &gt; bar\n</code></pre> <p>if you want to override its default value. You can do that in your <code>config.yaml</code>: <pre><code>VectorStoreConfig: &amp;VectorStoreConfig\n  # ... rest of the VectorStoreConfig ...\n  foo: baz\n</code></pre></p> <pre><code>print(rag.config.vector_store.foo)\n# &gt; baz\n</code></pre>"},{"location":"cookbook/loading_documents/","title":"How to load documents in the RAG","text":""},{"location":"cookbook/loading_documents/#loading-documents","title":"Loading documents","text":"<p>The easiest but least flexible way to load documents to your RAG is to use the <code>RAG.load_file</code> method. It will semi-intellignetly try to pick the best Langchain loader and parameters for your file.</p> <pre><code>from pathlib import Path\n\nfrom backend.rag_components.rag import RAG\n\n\ndata_directory = Path(\"data\")\n\nconfig_directory = Path(\"backend/config.yaml\")\nrag = RAG(config_directory)\n\nfor file in data_directory.iterdir():\n    if file.is_file():\n        rag.load_file(file)\n</code></pre> <p>If you want more flexibility, you can use the <code>rag.load_documents</code> method which expects a list of <code>langchain.docstore.document</code> objects.</p> <p>TODO: example</p>"},{"location":"cookbook/loading_documents/#document-indexing","title":"Document indexing","text":"<p>The document loader maintains an index of the loaded documents. You can change it in the configuration of your RAG at <code>vector_store.insertion_mode</code> to <code>None</code>, <code>incremental</code>, or <code>full</code>.</p> <p>Details of what that means here.</p>"},{"location":"cookbook/configs/databases_configs/","title":"Databases","text":"<p>The database config is the \"easiest\" as it only requires a database URL.</p> <p>So far, <code>sqlite</code>, <code>mysql</code>, and <code>postgresql</code> are supported.</p> <p>CloudSQL on GCP, RDS on AWS, or Azure Database will allow you to deploy <code>mysql</code>, and <code>postgresql</code> database instances.</p> <p>Warning</p> <p>If using <code>mysql</code> or <code>postgresql</code> you will need to also create a database, typically named <code>rag</code>, to be able to use it.</p> <p>You will also need to create a user, and get its password. Make sure there are no spacial characters in the password.</p> <p>As the database URL contains a username and password, we don't want to have it directly in the <code>config.yaml</code>.</p> <p>Instead, we have: <pre><code># backend/config.yaml\nDatabaseConfig: &amp;DatabaseConfig\n  database_url: {{ DATABASE_URL }}\n</code></pre></p> <p>And <code>DATABASE_URL</code> is coming from an environment variable.</p> <p>The connection strings are formated as follows:</p> <ul> <li> <p>SQLite: <code>sqlite:///database/rag.sqlite3</code> <pre><code>export DATABASE_URL=sqlite:///database/rag.sqlite3\n</code></pre></p> </li> <li> <p>mySQL: <code>mysql://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/rag</code> <pre><code># The typical port is 3306 for mySQL\nexport DATABASE_URL=mysql://username:abcdef12345@123.45.67.89:3306/rag\n</code></pre></p> </li> <li> <p>postgreSQL: <code>postgresql://&lt;username&gt;:&lt;password&gt;@&lt;host&gt;:&lt;port&gt;/rag</code> <pre><code># The typical port is 5432 for postgreSQL\nexport DATABASE_URL=postgresql://username:abcdef12345@123.45.67.89:5432/rag\n</code></pre></p> </li> </ul> <p>When first testing the RAG locally, <code>sqlite</code> is the best since it requires no setup as the database is just a file on your machine. However, if you're working as part of a team, or looking to industrialize, you will need to deploy a <code>mysql</code>, or <code>postgresql</code> instance.</p>"},{"location":"cookbook/configs/embedding_models_configs/","title":"Embedding Models","text":""},{"location":"cookbook/configs/embedding_models_configs/#locally-hosted-embedding-model-from-hugging-face","title":"Locally hosted embedding model from Hugging Face","text":"<p>This will download the selected model from the HF hub and make embeddings on the machine the backend is running on. <pre><code>pip install sentence_transformers\n</code></pre></p> <pre><code># backend/config.yaml\nEmbeddingModelConfig: &amp;EmbeddingModelConfig\n  source: HuggingFaceEmbeddings\n  source_config:\n    model_name : 'BAAI/bge-base-en-v1.5'\n</code></pre>"},{"location":"cookbook/configs/embedding_models_configs/#artefact-azure-hosted-embedding-model","title":"Artefact Azure-hosted embedding model","text":"<pre><code># backend/config.yaml\nEmbeddingModelConfig: &amp;EmbeddingModelConfig\n  source: OpenAIEmbeddings\n  source_config:\n    openai_api_type: azure\n    openai_api_key: {{ EMBEDDING_API_KEY }}\n    openai_api_base: https://poc-openai-artefact.openai.azure.com/\n    deployment: embeddings\n    chunk_size: 500\n</code></pre>"},{"location":"cookbook/configs/embedding_models_configs/#aws-bedrock","title":"AWS Bedrock","text":"<p>You will first need to login to AWS</p> <p><pre><code>pip install boto3\n</code></pre> Follow this guide to authenticate your machine</p> <pre><code># backend/config.yaml\nEmbeddingModelConfig: &amp;EmbeddingModelConfig\n  source: BedrockEmbeddings\n  source_config:\n    model_id: 'amazon.titan-embed-text-v1'\n</code></pre>"},{"location":"cookbook/configs/llms_configs/","title":"LLMs","text":""},{"location":"cookbook/configs/llms_configs/#artefact-azure-hosted-gpt4-turbo","title":"Artefact Azure-hosted GPT4-turbo","text":"<pre><code># backend/config.yaml\nLLMConfig: &amp;LLMConfig\n  source: AzureChatOpenAI\n  source_config:\n    openai_api_type: azure\n    openai_api_key: {{ OPENAI_API_KEY }}\n    openai_api_base: https://genai-ds.openai.azure.com/\n    openai_api_version: 2023-07-01-preview\n    deployment_name: gpt4\n    temperature: 0.1\n</code></pre>"},{"location":"cookbook/configs/llms_configs/#local-llama2","title":"Local llama2","text":"<p>You will first need to install and run Ollama</p> <p>Download the Ollama application here</p> <p>Ollama will automatically utilize the GPU on Apple devices.</p> <pre><code>ollama run llama2\n</code></pre> <pre><code># backend/config.yaml\nLLMConfig: &amp;LLMConfig\n  source: ChatOllama\n  source_config:\n    model: llama2\n</code></pre>"},{"location":"cookbook/configs/llms_configs/#vertex-ai-gemini-pro","title":"Vertex AI gemini-pro","text":"<p>Warning</p> <p>Right now Gemini models' safety settings are very sensitive, and is is not possible to disable them. That makes this model pretty much useless for the time being as it blocks most requests and/or responses.</p> <p>Github issue to follow: https://github.com/langchain-ai/langchain/pull/15344#issuecomment-1888597151</p> <p>You will first need to login to GCP</p> <pre><code>export PROJECT_ID=&lt;gcp_project_id&gt;\ngcloud config set project $PROJECT_ID\ngcloud auth login\ngcloud auth application-default login\n</code></pre> <p>Activate the Vertex APIs in your project</p> <pre><code># backend/config.yaml\nLLMConfig: &amp;LLMConfig\n  source: ChatVertexAI\n  source_config:\n    model_name: gemini-pro\n    temperature: 0.1\n</code></pre>"},{"location":"cookbook/configs/vector_stores_configs/","title":"Vector Stores","text":""},{"location":"cookbook/configs/vector_stores_configs/#postgresql","title":"PostgreSQL","text":"<p>As we need a backend SQL database to store conversation history and other info, using Postgres as a vector store is very attractive for us. Implementeing all this functionalities using the same technology reduces deployment overhead and complexity.</p> <p>See the recipes for database configs here</p> <pre><code>pip install psycopg2-binary pgvector\n</code></pre> <pre><code># backend/config.yaml\nVectorStoreConfig: &amp;VectorStoreConfig\n  source: PGVector\n  source_config:\n    connection_string: {{ DATABASE_URL }}\n\n  retriever_search_type: similarity_score_threshold\n  retriever_config:\n    k: 20\n    score_threshold: 0.5\n\n  insertion_mode: null\n</code></pre> <p><code>top_k</code>: maximum number of documents to fetch.</p> <p><code>score_threshold</code>: score below which a document is deemed irrelevant and not fetched.</p> <p><code>insertion_mode</code>: <code>null</code> | <code>full</code> | <code>incremental</code>. How document indexing and insertion in the vector store is handled.</p>"},{"location":"cookbook/configs/vector_stores_configs/#local-chroma","title":"Local Chroma","text":"<pre><code># backend/config.yaml\nVectorStoreConfig: &amp;VectorStoreConfig\n  source: Chroma\n  source_config:\n    persist_directory: vector_database/\n    collection_metadata:\n      hnsw:space: cosine\n\n  retriever_search_type: similarity_score_threshold\n  retriever_config:\n    k: 20\n    score_threshold: 0.5\n\n  insertion_mode: null\n</code></pre> <p><code>persist_directory</code>: where, locally the Chroma database will be persisted.</p> <p><code>hnsw:space: cosine</code>: distance function used. Default is <code>l2</code>. Cosine is bounded [0; 1], making it easier to set a score threshold for retrival.</p> <p><code>top_k</code>: maximum number of documents to fetch.</p> <p><code>score_threshold</code>: score below which a document is deemed irrelevant and not fetched.</p> <p><code>insertion_mode</code>: <code>null</code> | <code>full</code> | <code>incremental</code>. How document indexing and insertion in the vector store is handled.</p>"},{"location":"deployment/admin_mode/","title":"Admin Mode","text":"<p>Once you start deploying to a cloud, you may end up having to publicly expose your backend and frontend to the internet. In this case you will need to disable the admin mode on the deployed components.</p> <p>Just deploy with the <code>ADMIN_MODE</code> env var as disabled. This could be in your Dockerfile, or any other deployment config. <pre><code>ADMIN_MODE=0\n</code></pre></p> <p>This will disable the signup endpoint, preventing people stumbling upon the API from creating an account and using your LLM tokens.</p>"}]}