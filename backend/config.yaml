LLMConfig: &LLMConfig
  source: ChatOllama
  source_config:
    model: tinyllama
    temperature: 0
    # base_url: http://host.docker.internal:11434  # Uncomment this line if you are running the RAG through Docker Compose

VectorStoreConfig: &VectorStoreConfig
  source: Chroma
  source_config:
    persist_directory: vector_database/
    collection_metadata:
      hnsw:space: cosine

  insertion_mode: null

EmbeddingModelConfig: &EmbeddingModelConfig
  source: HuggingFaceEmbeddings
  source_config:
    model_name: BAAI/bge-base-en-v1.5
    chunk_size: 500

DatabaseConfig: &DatabaseConfig
  database_url: sqlite:///database/rag.sqlite3

RagConfig:
  llm: *LLMConfig
  vector_store: *VectorStoreConfig
  embedding_model: *EmbeddingModelConfig
  database: *DatabaseConfig
  chat_history_window_size: 5
  max_tokens_limit: 3000
  response_mode: stream
